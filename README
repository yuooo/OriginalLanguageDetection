Building the features to train upon:
The parser is located in a class called Main. It is in directory src/main, in package java.mess.Main. It takes no arguments,
as the data we are loading from is just hardcoded, as the directory structure must be exact in order for our code to be correct.

To compile it, from this directory, run:

javac -cp "Jars/*:" -d out src/main/java/mess/Main.java src/main/java/mess/Features/* src/main/java/mess/utils/* src/main/java/mess/Algorithm/*


Then to run it, run:

java -cp "Jars/*:out/:" mess.Main


The code will run on Weka, generating Lexical Features, Parse Features, and Homemade Features... see report for more details on each one.
Once these features are computed, we merge them all together. We don't use the Weka toolkit to classify things in Java, we use
Python to do the actual machine learning (see below for that info). Run:

pip install xgboost
pip install sklearn
python src/main/python/analytics.py

To obtain the training/testing accuracy of 4 classifiers (Logistic Regression, Random Forest, XGBoost, SVM), as well as the confusion matrix. The list of the 100 most important class features for LR is also printed.

In case there are difficulties installing xgboost, a xgboost free version also exist:

pip install sklearn
python src/main/python/analytics_without_xgb.py


PREPROCESSING CODE (Not as important, but still around in case you want to run it)

TO CLEAN TXT: Run txt/mattCleanTxt.sh.
txt/mattCleanTxt.sh file.txt [regex] [regex] ...
Arguments:
file.txt is the file you want cleaned.
regex is a regular expression for a line that is replaced by blank. used for chapter header removal, and also can remove subtitles to chapter text as well.


MASSPREPROCESSOR
MassPreprocessor: This flexible class takes a novel in text format and prints out either one of two things (or possibly both):
    -a tokenized text file (one sentence per line), or
    -a Penn tree file, one sentence per tree.

NOTE: Running -tree on a whole novel can take a very long time. Hence I've included a condor script called condor_more_trees and a script I used to build it, condor_build_trees.sh, to show a practical use for it.

Usage: MassPreprocessor [-text] [-tree] [-oneNovel novelName] [-textDirectory directory] [-treeDirectory directory]  [rootDirectory]

Arguments: (all are optional except for rootDirectory, but you won't get very far without stating either -text or -tree.
        -text: returns text files.
        -tree: returns tree files.
            Both -text and -tree can be called at the same time.
        -oneNovel novel_file: runs only for novel_file. Please include the path in the name (Local or absolute path names are both OK). Also rootDirectory will not be read anymore.
        -textDirectory directory: Text output will go into the Data/directory directory instead of the default directory created by the code (txt_sentence_blocks_merged).
        -treeDirectory directory: Tree output will go into the Data/directory directory instead of the default directory created by the code (trees_merged).
            NOTE: Only a name must be provided for -textDirectory and -treeDirectory. DO NOT provide the whole directory, otherwise it won't work.
            These will both write into
        rootDirectory the directory in which files are to be converted.
            If doing a batch job, Subdirectories of rootDirectory need to have language names, and in those lie the novels.
            If running for just one novel, rootDirectory MUST be the language in which the novel originates.


SPLITFILE
SplitFile:

Allows a user to split either text files or tree files. Both use \n to delimit, but differ slightly in delimitation:
      Text uses \n to delimit per sentence
      Tree delimits using \n as the only character on the line to note that a tree has finished.

User can indicate number of files they want using -trainnum # and -testnum # to determine. The sum of those make the number of files extracted.
      Defaults are 19 and 1 for train and test, respectively.
      The train files will be written first, then the test files will be written second. Example, with 19 train files and 1 test file, our code writes 19 files first for training, then 1 for testing.

 The command should look something like this (hyphenated command order doesn't matter... BUT must have options, then root_directory must be first and destination_directory must be second!)

      SplitFile -tree/-text [-seednum #] [-trainall/-trainnum #] [-testall/-testnum #] source_directory destination_directory

Arguments explained below.

-tree/text: Need to call either -tree or -text or neither will execute. Can't call both in one run, otherwise you get an error message.
      -text uses \n to delimit per sentence
      -tree delimits using \n as the only character on the line to note that a Penn tree has finished.  

-seednum: Set the number of seeds per file by using the -seeds option. Default is 100.

-trainall: Sets number of train files to be generated to an impractical number, forcing the code to exit when the reader is finished reading lines.
    Also throws away the last file as it is most likely incomplete.
-trainnum: Sets the number of files to write to create our training set, per novel. Default is 0.


source_directory: Needs to have languages in subdirectories, and these languages contain the novels in text or tree format.

destination_directory: Will write into this directory (it doesn't need to exist yet) train/ and test/ directories. From each, the languages, and all chunks of all novels will be written in there.


The Rest of our code that we wrote:
Algorithm.Classifier_OLI: The classifier that we use for this project.
Features package: We have the Features Abstract Class, and our three sets of features discussed in our paper: Homemade, Lexical, and Parse.
preprocessing package: Elements explained above, excpet ParseWikitionary, which helps us compute etymology.
utils: Useful classes in here. POS is an enum used to compute HomemadeFeatures. TextPOSTreeTriple, for one sentence, returns a tree, POS, and text variations. TreetoSentenceHandler is a wrapper for the Stanford Parser treebank, and Utils has various utility functions used in Main.