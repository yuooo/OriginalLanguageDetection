\documentclass[10pt]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{url}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{wasysym}
\usepackage{amssymb }
\usepackage{caption}
\usepackage{tikz}

%\usepackage{fontspec}
%\setmainfont{Hoefler Text}

\newcommand*\Let[2]{\State #1 $\gets$ #2}
%\algrenewcommand\alglinenumber[1]{
   % {\sf\footnotesize\addfontfeatures{Colour=888888,Numbers=Monospaced}#1}}
\algrenewcommand\algorithmicrequire{\textbf{Precondition:}}
\algrenewcommand\algorithmicensure{\textbf{Postcondition:}}

\RequirePackage[l2tabu, orthodox]{nag}


\usepackage{amsmath,amssymb,amsthm}

\DeclareMathOperator*{\E}{\mathbb{E}}
\let\Pr\relax
\DeclareMathOperator*{\Pr}{\mathbb{P}}

\newcommand{\eps}{\epsilon}
\newcommand{\inprod}[1]{\left\langle #1 \right\rangle}
\newcommand{\R}{\mathbb{R}}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf CS 388: Natural Language Processing} \hfill #2 }
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em #3 \hfill #4} }
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\ps}[4]{\handout{#1}{#2}{Students: #4}{#3}{Project Report #1}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex

\renewcommand\thesubsection{(\alph{subsection})}

%%% START

\begin{document}

\ps{--- Original Language Detection}{Spring 2016}{Prof.\ Ray Mooney}{M. Denend, J. Hoffmann, L.Prakash}
\begin{abstract}
Abstract goes here.
\end{abstract}
\section{Introduction}
Our task was to determine the original language of translated text. Determining the language of origin for written text is something that may potentially have applications for national security, for example, if terrorists use social media. Also, it may be of great importance to historians in order to trace back the path in which ancient novels may have been translated from, for example, the Bible. Doing so and using improving machine translation to translate a book back into prior languages may possibly open up avenues of research to better understand intended meanings of the first authors, as meaning can get lost when novels are translated. Furthermore, previous studies have shown that machine translators are much more accurate in identifying whether writing was from an original language, or translated, as mentioned in (I KNOW WHICH PAPER, I JUST NEED TO ACTUALLY CITE IT).
Our goal was to build upon past research. In particular, we found that \cite{homemade} had some good results when trying to determine the original language, however, it only tested against slices of the same books, which we felt defeats the potential for applying to other books. So we decided to have our train set and test set be from completely separate novels, which is one of our major contributions. We liked the idea of using horizontal slices of parse trees mentioned in \cite{Parse}, so we decided to take some features from \cite{homemade} and add parse features to this. We built our dataset in a manner like in \cite{homemade}, but decided to extend it by adding a couple more books and two more languages. Our results were... (Jessica?)
\section{Problem Definition}
\subsection{Description}
Our project is to take novels, freely available from Project Gutenberg (CITE THIS), and translated from a wide variety of languages, and determine which language they originated from. To do this, we take a novel electronically available in plain text format, and run some algorithms to determine various features that we detect from the books. We group these into three types of features (to be elaborated upon more later): lexical features, parse features (horizontal slices), and homemade features. Once we've computed these features for both our train and test sets, we feed them to a machine learning algorithm to see if it can determine the source language for the novels in the test set. 

\subsection{Algorithm}
We take a text format novel from Project Gutenberg. We clean out of it Chapter headings, subheaders following chapters, beginning and end informational text, and anything else that isn't words that we can use to compute. From here, we use the Stanford Parser's PCFG (CITE THIS) to build best-guess parse trees (in Penn Tree format) for the sentences in this novel, and we use the Stanford Parser's tokenizer to build tokenized sentences and detect ends of sentences. Both the tree format and text format are used in our machine learning, but we could just use the tree if space efficiency was a problem. The text format is used to compute word unigrams (using Weka's toolkit to do so), and the tree format is used to compute our parse features and homemade features as well. Both the parse and homemade feature computations were built from scratch with the ideas from  \cite{homemade} and  \cite{Parse}, with some help from Stanford's Parser package to read the Penn Trees. Finally, all the features were combined together and trained/tested upon using our (which?) machine learning algorithm.

\subsection{Methodology}
From Project Gutenberg, we built a dataset of books written in different languages, and translated into English. We then cut these books into slices of 400 sentences, and compute features on these slices. After feature extraction, we feed these features into a classifier and test it on a completely different set of books. 

\section{Building the dataset}
\subsection{Pitfalls}
Since we built it ourselves, our dataset is rather small. We therefore were extremely careful not to overfit on books particularities that are not specific to the language, such as topic, style, author, geographic places, time period. We believe the way we've constructed the dataset satisfies these criteria. 
\subsection{Description of the dataset}
When building the dataset, we used books written from 1700 to 1920 to control the time period as best as we can. We made sure that we never used the same author nor translator (?) in train and test, and we tried varying the topics between train and test as much as we could once the previous criteria were met. When using Lexical Features, we only remembered the most common 1000 English words to control overfitting on topics as well.
The dataset we used is further described on this table:



\begin{tabular}{|c|c|c|c|}
\hline 
Language & Title & Author & Pub.Date\tabularnewline
\hline 
\hline 
American & The Adventures of Huckleberry Finn & Mark Twain & 1884\tabularnewline
\hline 
American & The Invisible Man  & Orson Wells & 1897\tabularnewline
\hline 
American & The Scarlet Letter & Nathaniel Hawthorne & 1850\tabularnewline
\hline 
American & Moby Dick & Herman Melville & 1851\tabularnewline
\hline 
American & Uncle Toms Cabin & Harriet Beecher Stowe & 1852\tabularnewline
\hline 
American & The Narrative of A. G. Pym of Nantucket & Edgar Allen Poe & 1899\tabularnewline
\hline 
\hline 
English & Great Expectations & Charles Dickens & 1861\tabularnewline
\hline 
English & The Picture of Dorian Gray & Oscar Wilde & 1891\tabularnewline
\hline 
English & Jude the Obscure & Thomas Hardy & 1895\tabularnewline
\hline 
English & Treasure Island & R.L Stevenson & 1883\tabularnewline
\hline 
English & Middlemarch & George Eliot (M. Evans) & 1874\tabularnewline
\hline 
English & Frankenstein & Mary Wollstonecraft Shelley & 1818\tabularnewline
\hline
\hline  
French & Around the world in 80 days & Jules Verne  & 1873\tabularnewline
\hline 
French & Father Goriot  & Balzac & 1835\tabularnewline
\hline 
French & Les Miserables  & Victor Hugo & 1862\tabularnewline
\hline 
French & Madame Bovary & Gustave Flaubert  & 1857\tabularnewline
\hline 
French & The Man in the Iron Mask & Alexandre Dumas & 1637\tabularnewline
\hline 
French & Candide & Voltaire & 1759\tabularnewline
\hline 
French & The Red and the Black & Stendahl & 1830\tabularnewline
\hline 
\hline 
Russian & Fathers and Children  & Ivan Turgenev  & 1862\tabularnewline
\hline 
Russian & The Man who was afraid  & Maxim Gorky  & 1899\tabularnewline
\hline 
Russian & War and Peace & Leo Tolstoy & 1869\tabularnewline
\hline 
Russian & The Brothers Karamazov & Fyodor Dostoyevsky & 1880\tabularnewline
\hline 
Russian & A hero of Our Time  & M. Y. Lermontov & 1840\tabularnewline
\hline 
Russian & The Death of the Gods & Dmitri Mérejkowski & 1895\tabularnewline
\hline 
\hline 
Spanish & Don Quixote & Miguel de Cervantes & 1605\tabularnewline
\hline 
Spanish & Dona Perfecta & Benito Pérez Galdós & 1876\tabularnewline
\hline 
Spanish & The Visions of Quevedo & Francisco de Quevedo & 1626\tabularnewline
\hline 
Spanish & Tragic Sense of Life & Miguel de Unamuno & 1912\tabularnewline
\hline 
Spanish & The Life of Lazarillo of Tormes & Lazarillo of Tormes & 1554\tabularnewline
\hline 
Spanish & The Four Horsemen of the Apocalypse & Vicente Blasco Ibanez & 1916\tabularnewline
\hline 
Spanish & Pepita Ximenez & Juan Valera & 1874\tabularnewline
\hline 
Spanish & The Fourth Estate & Armando Palacio Valdés & 1901\tabularnewline
\hline 
\hline 
German & The Devil's Elixir & E. T. A. Hoffmann & 1815\tabularnewline
\hline 
German & Debit and Credit & Gustave Freytag  & 1855\tabularnewline
\hline 
German & Old Fritz and the New Era & Luise Mühlbach & 1868\tabularnewline
\hline 
German & Venus in Furs & Leopold V. Sacher-Masoch  & 1870\tabularnewline
\hline 
German & The Sorrows of Young Werther & Johann Wolfgang von Goethe & 1774\tabularnewline
\hline 
German & The Banished: A Swabian Historical Tale & Wilhelm Hauff & 1839\tabularnewline
\hline 
\end{tabular}

\section{Features}
\subsection{General Description}
When computing the features, we cut the books in slices of $n$ sentences. With $n=1$, we get a lot of training examples, but finding out the original language from only 1 sentence seems unreasonable (which was proven by experiment). With the whole book as a sample, we didn't get enough training instances. We chose $n=400$, as it provided good balance between number of train samples, and informativeness of the samples. In training, we take 8 slices of each book, so that we have the same size of training for each language. In testing, we take all the slices, except the incomplete last one.
\subsection{Lexical Feature}
Three different kind of lexical features were used:
\begin{description}
\item[1000 most common English words] After overfitting on words like "Moscow" or "Madame" when using unigrams, we decided to only use the 1000 most common words in English to avoid overfitting on topics, local idioms, or geographic places.
\item[POS unigram] We only used POS unigrams, since we believed using bigram or trigram would just add redundancy with the Parse features.
\item[Etymology] For each word used, we computed the etymology of the word from parsing Wiktionary \cite{Wiktionary}, and added all possible etymology as a feature. Rational being that if you're translating from a latin language, you may be more likely to use a latin word when choosing between synonyms. We only included the etymologies of nouns, verb, adjective and adverb, since we believed preposition, pronouns  and others wouldn't be significant and would only drown the relevant data.
\end{description}

\subsection{Parse Features}
Inspired by \cite{Parse}, we ran the Stanford parser on our slices, and computed the parse trees. By running a BFS, we then compute the generation rules and count them. We don't take into account the leaves of the trees to avoid redundancy with lexical features (and possible topic overfitting).


\subsection{Homemade Features}
We also computed all the features of \cite{homemade}, which include length of sentences, length of words, ratio of pronouns, nouns, complex sentences etc. [MATT ADD WHATEVER YOU WANT].

\section{Classification task}
\subsection{Features Extraction}
\subsection{Comparisons of different algorithm}

\section{Experiments}
\subsection{Description of different experiments}
We conducted the following experiments:
\begin{itemize}
\item using unigrams (overfitting on topics)
\item using the same books for train and test
\end{itemize}

On top of that, we also inquired the effect of different features on the dataset:
\begin{itemize}
\item Most common words only
\item Most common + etymology
\item POS only
\item All lexical features (Most common words + etymology + POS)
\item Homemade only
\item Parse only
\item Most common + etymology + Homemade
\item All features
\end{itemize}

\subsection{Results}
\begin{figure}[h]
\centering\begin{tabular}{|c|c|c|c|}
\hline 
Experiment & Accuracy & Best classifier\\
\hline 
1000 Most common words & 96.67 \% & LR \\
\hline
\end{tabular}
\caption{Testing and training on the same books (different parts)}
\end{figure}
As expected, using the same books for training and testing gives excellent results even when using only the 1000 most common English words. Since we used a lot of books in common with \cite{homemade}, we're wondering how they got such poor results.

\begin{figure}[h]
\centering\begin{tabular}{|c|c|c|c|}
\hline 
Experiment & Accuracy \\
\hline 
Unigrams & 61.80 \% \\
1000 Most common words & 62.92 \%  \\
1000 Most common words + etymology & 61.80 \%  \\
1000 Most common words + POS & 61.80 \%  \\
\hline
Parse features only & \\
\hline
\end{tabular}
\caption{Lexical Features}
\end{figure}
We get better results when using only the 1000 most common English words, as opposed to all the words (Unigrams). This was a very positive result, since it suggests that we did built our dataset in a robust way. Indeed, using unigrams completely overfitted on topic, as illustrated by the following example: we used Moby Dick in our training, and therefore \emph{whale} is the third most relevant feature for American books. Contrary to intuition, the etymology feature didn't help at all.


\section{Conclusion}

\bibliographystyle{plain}
\bibliography{OLI}

\end{document}
